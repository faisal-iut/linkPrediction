{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import operator as op\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy.linalg\n",
    "import cmath\n",
    "import multiprocessing\n",
    "import itertools\n",
    "import scipy.optimize as optimize\n",
    "from tabulate import tabulate\n",
    "from documentCentrality import document_centrality\n",
    "from networkx import algorithms as ag\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keyword_split(df,key):\n",
    "    df[key]= df[key].str.split(\"; \", n = 20, expand = False)  \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def node_key_find(kl,label):\n",
    "    key=kl[kl['keyword']==label]['id'].iloc[0]\n",
    "    return key\n",
    "\n",
    "def node_label_find(kl,key):\n",
    "    label=kl[kl['id']==key]['keyword'].iloc[0]\n",
    "    return label    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nodes_intersection(df,kl,t0,t1,t2):\n",
    "    prelist=[]\n",
    "    postlist=[]\n",
    "    pre_df=df[(df['art_year']>=t0)&(df['art_year']<t1)]\n",
    "    post_df=df[(df['art_year']>=t1)&(df['art_year']<t2)]\n",
    "    for index, row in pre_df.iterrows():\n",
    "        for label in row[\"keyword\"]:\n",
    "            node_id=node_key_find(kl,label)\n",
    "            prelist.append(node_id)\n",
    "    for index, row in post_df.iterrows():\n",
    "        for label in row[\"keyword\"]:\n",
    "            node_id=node_key_find(kl,label)\n",
    "            postlist.append(node_id)\n",
    "    return set(prelist).intersection(set(postlist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_graph(g,df,kl,nodes,t0,t1):\n",
    "    g_df=df[(df['art_year']>=t0)&(df['art_year']<t1)]\n",
    "    #node insert\n",
    "    for index, row in g_df.iterrows():\n",
    "        for label in row[\"keyword\"]:\n",
    "            node_id=node_key_find(kl,label)\n",
    "            if node_id in nodes:\n",
    "                if g.has_node(node_id):\n",
    "                    g.nodes[node_id]['art_id'].add(row[\"art_id\"])\n",
    "                    g.nodes[node_id]['year'].add(row[\"art_year\"])\n",
    "                    g.nodes[node_id]['title'].add(row[\"title\"])\n",
    "                else:\n",
    "                    art_id={row[\"art_id\"]}\n",
    "                    year={row[\"art_year\"]}\n",
    "                    title={row[\"title\"]}\n",
    "                    g.add_node(node_id,art_id=art_id,year=year,title=title)\n",
    "    #edge insert\n",
    "    for index, row in g_df.iterrows():\n",
    "        edges=list(itertools.combinations(row[\"keyword\"], 2))\n",
    "        for edge in edges:\n",
    "            node1=node_key_find(kl,edge[0])\n",
    "            node2=node_key_find(kl,edge[1])\n",
    "            if (node1 in nodes) and (node2 in nodes) and (node1!=node2):\n",
    "                if not g.has_edge(node1,node2):\n",
    "                    g.add_edge(node1,node2)                \n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "def node_and_article_feature(df,kl,g,t0,t1):\n",
    "    f_df=df[(df['art_year']>=t0)&(df['art_year']<t1)]\n",
    "    article_set=[]\n",
    "    node_set=[]\n",
    "    for nd,art in g.node(data='art_id'):\n",
    "        node_set.append(nd)\n",
    "        for art_s in art:\n",
    "            article_set.append(art_s)\n",
    "    article_set= set(article_set)\n",
    "    print(len(article_set))\n",
    "    node_set= set(node_set)\n",
    "    article_index=list(article_set)\n",
    "    node_index=list(node_set)\n",
    "    td=np.zeros((len(g.nodes()), len(article_set)))\n",
    "    for nd,art in g.node(data='art_id'):\n",
    "        for art_s in art:\n",
    "            td[node_index.index(nd)][article_index.index(art_s)]=1\n",
    "                \n",
    "    #document centrality feature \n",
    "    ca,cn=document_centrality(td,20)\n",
    "    node_feature=pd.DataFrame({'node_index':node_index,'term_centrality':cn})\n",
    "    article_feature=pd.DataFrame({'article_index':article_index,'article_centrality_on_node':ca})\n",
    "    return node_feature,article_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_data(df,kl,nodes,g_train,g_test):\n",
    "    train_labels=[]\n",
    "    train_rows = list(nx.non_edges(g_train))\n",
    "    for edge in train_rows:\n",
    "        if g_test.has_edge(edge[0],edge[1]):\n",
    "            train_labels.append(1)\n",
    "        else:\n",
    "            train_labels.append(0)\n",
    "    train_data = pd.DataFrame({'row_name':train_rows,'label':train_labels})\n",
    "    test_data = pd.DataFrame({'row_name':list(set(g_test.edges()))})\n",
    "    return train_data,test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_feature_set(df,kl,g,feature_frame,t0,t1):\n",
    "    #node and article feature\n",
    "    node_feature,article_feature = node_and_article_feature(df,kl,g,t0,t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('dataset/apnea-all,3.csv')\n",
    "key_list = pd.read_csv('dataset/apnea-distinct_keyword.csv')\n",
    "article_keywords = pd.read_csv('dataset/apnea-art_id,keyword,key_id,3.csv')\n",
    "sm_df=df[(df['art_year']<2020)]\n",
    "sm_df=keyword_split(sm_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0=1991\n",
    "t1=1992\n",
    "t2=1993\n",
    "nodes=nodes_intersection(sm_df,key_list,t0,t1,t2)\n",
    "g=nx.Graph()\n",
    "g_train=build_graph(g,sm_df,key_list,nodes,t0,t1)\n",
    "g=nx.Graph()\n",
    "g_test=build_graph(g,sm_df,key_list,nodes,t1,t2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31 20 36 36 36\n"
     ]
    }
   ],
   "source": [
    "print(len(g_train.edges()),len(g_test.edges()),len(g_train.nodes()),len(g_test.nodes()),len(nodes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67\n",
      "599 11\n"
     ]
    }
   ],
   "source": [
    "train_data,test_data=classification_data(sm_df,key_list,nodes,g_train,g_test)\n",
    "build_feature_set(sm_df,key_list,g_train,train_data,t0,t1)\n",
    "print(len(train_data),len(train_data[(train_data['label']==1)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "196350 / 325583\n"
     ]
    }
   ],
   "source": [
    "st=0\n",
    "end=330\n",
    "cut=4\n",
    "tds=train_data\n",
    "coun2=0\n",
    "coun=[]\n",
    "for i,j in tds.iterrows():\n",
    "    coun1=0\n",
    "    nb1=set(nx.neighbors(g_train,j[0][0]))\n",
    "    nb2=set(nx.neighbors(g_train,j[0][1]))\n",
    "    nb1h2=nb1.copy()\n",
    "    nb2h2=nb2.copy()\n",
    "    for n2b1 in nb1h2:\n",
    "        nb=set(nx.neighbors(g_train,n2b1))\n",
    "        nb1.update(nb)\n",
    "    for n2b2 in nb2h2:\n",
    "        nb=set(nx.neighbors(g_train,n2b2))\n",
    "        nb2.update(nb)    \n",
    "    for nnb1 in nb1:\n",
    "        for nnb2 in nb2:\n",
    "            if(nnb1==nnb2):\n",
    "                coun1=coun1+1\n",
    "    if coun1>0:\n",
    "        coun2=coun2+1\n",
    "    coun.append(coun1)  \n",
    "print(coun2,\"/\",len(tds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st=0\n",
    "end=100000\n",
    "cut=4\n",
    "tds=train_data[(train_data['label']==0)]\n",
    "tds=tds[0:end]\n",
    "coun2=0\n",
    "for i,j in tds.iterrows():\n",
    "    coun1=0\n",
    "    nb1=set(nx.neighbors(g_train,j[0][0]))\n",
    "    nb2=set(nx.neighbors(g_train,j[0][1]))\n",
    "    nb1h2=nb1.copy()\n",
    "    nb2h2=nb2.copy()\n",
    "    for n2b1 in nb1h2:\n",
    "        nb=set(nx.neighbors(g_train,n2b1))\n",
    "        nb1.update(nb)\n",
    "    for n2b2 in nb2h2:\n",
    "        nb=set(nx.neighbors(g_train,n2b2))\n",
    "        nb2.update(nb)\n",
    "    for nnb in nb1:\n",
    "        nnb1=set(nx.neighbors(g_train,nnb))\n",
    "        for nnnb in nnb1:\n",
    "            nnnb1=set(nx.neighbors(g_train,nnnb))\n",
    "            if(j[0][1] in nnnb1):\n",
    "                coun1=coun1+1\n",
    "    if coun1>0:\n",
    "        coun2=coun2+1\n",
    "print(coun2,\"/\",len(tds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.1666666666666665 ---- cut: 3 ----- avg: 8050.666666666667 ----------\n",
      "3.5757575757575757 ---- cut: 4 ----- avg: 102718.75 ----------\n"
     ]
    }
   ],
   "source": [
    "st=0\n",
    "end=330\n",
    "cut=5\n",
    "tds=train_data[(train_data['label']==1)]\n",
    "#tds=tds[0:end]\n",
    "for jj in range(3,cut):\n",
    "    count=0\n",
    "    total=0\n",
    "    for i,j in tds.iterrows():\n",
    "        paths = list( nx.all_simple_paths(g_train, source=j[0][0], target=j[0][1],cutoff=jj))\n",
    "        nb1=set(nx.neighbors(g_train,j[0][0]))\n",
    "        if len(paths)!=0:\n",
    "            #print(j[0][0],'+',j[0][1],\"----\",len(paths))\n",
    "            count=count+1\n",
    "            total=total+len(paths)\n",
    "    print(count,\"/\",end,\"----\",\"cut:\",jj,\"-----\",\"avg:\",total/(jj),\"----------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5909090909090908 ---- cut: 3 ----- avg: 21.666666666666668 ----------\n"
     ]
    }
   ],
   "source": [
    "end=4*330\n",
    "tds=train_data[(train_data['label']==0)]\n",
    "tds=tds[3*330:end]\n",
    "for jj in range(3,cut):\n",
    "    count=0\n",
    "    total=0\n",
    "    for i,j in tds.iterrows():\n",
    "        paths = list( nx.all_simple_paths(g_train, source=j[0][0], target=j[0][1],cutoff=jj))\n",
    "        if len(paths)!=0:\n",
    "            #print(j[0][0],'+',j[0][1],\"----\",len(paths))\n",
    "            count=count+1\n",
    "            total=total+len(paths)\n",
    "    print(count/end,\"----\",\"cut:\",jj,\"-----\",\"avg:\",total/(jj),\"----------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [],
   "source": [
    "tds=train_data[(train_data['label']==0)]\n",
    "cm_neigh_len=[]\n",
    "cm_neigh_list=[]\n",
    "for i,j in train_data.iterrows():\n",
    "    coun=0\n",
    "    cm = list(nx.common_neighbors(g_train, j[0][0],j[0][1]))\n",
    "    cm_neigh_len.append(len(cm))\n",
    "    cm_neigh_list.append(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['cm']=cm_neigh_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "st=0\n",
    "end=10000\n",
    "cut=4\n",
    "tds=train_data[(train_data['label']==0)]\n",
    "#tds=tds[0:end]\n",
    "coun2=0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discard_negative(train_data,g_train):\n",
    "    #pref attach\n",
    "    pref = nx.preferential_attachment(g_train,list(train_data['row_name']))\n",
    "    pref = list(pref)\n",
    "    train_data['pref']=np.array(pref)[:,2]\n",
    "    cm_neigh_len=[]\n",
    "    cm_neigh_list=[]\n",
    "    for i,j in train_data.iterrows():\n",
    "        coun=0\n",
    "        cm = list(nx.common_neighbors(g_train, j[0][0],j[0][1]))\n",
    "        cm_neigh_len.append(len(cm))\n",
    "        cm_neigh_list.append(cm)\n",
    "    train_data['cm']=cm_neigh_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "pref = nx.preferential_attachment(g_train,list(train_data['row_name']))\n",
    "pref = list(pref)\n",
    "train_data['pref']=np.array(pref)[:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_neigh_len=[]\n",
    "cm_neigh_list=[]\n",
    "for i,j in train_data.iterrows():\n",
    "    coun=0\n",
    "    cm = list(nx.common_neighbors(g_train, j[0][0],j[0][1]))\n",
    "    cm_neigh_len.append(len(cm))\n",
    "    cm_neigh_list.append(cm)\n",
    "train_data['cm']=cm_neigh_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['cm_hop2']=coun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['soundara']=np.array(soundara)[:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6943 296\n"
     ]
    }
   ],
   "source": [
    "l1= len(train_data[((train_data['pref']>0)&(train_data['cm']>1)&(train_data['cm_hop2']>0))& (train_data['label']==0)])\n",
    "l2= len(train_data[((train_data['pref']>0)&(train_data['cm']>1)&(train_data['cm_hop2']>0))& (train_data['label']==1)])\n",
    "print(l1,l2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1180"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data[(train_data['cm_hop2']>0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1980"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm_df['art_year'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "osteotomy + surgery\n",
      "[10248, 10248]\n",
      "----------------------------------------------------\n",
      "osteotomy + distraction\n",
      "[10248, 10248]\n",
      "----------------------------------------------------\n",
      "osteotomy + maxilla\n",
      "[10248, 10248]\n",
      "----------------------------------------------------\n",
      "osteotomy + sleep\n",
      "[10248, 10248]\n",
      "----------------------------------------------------\n",
      "osteotomy + apert syndrome\n",
      "[10248, 10248]\n",
      "----------------------------------------------------\n",
      "osteotomy + obstructive\n",
      "[10248, 10248]\n",
      "----------------------------------------------------\n",
      "malocclusion + overjet\n",
      "[8202]\n",
      "----------------------------------------------------\n",
      "sleep position + children\n",
      "[14349, 14349, 14349, 14349, 14349]\n",
      "----------------------------------------------------\n",
      "sleep position + supine position\n",
      "[14349, 14349, 14349, 14349, 14349]\n",
      "----------------------------------------------------\n",
      "paediatric + morbid obesity\n",
      "[14348]\n",
      "----------------------------------------------------\n",
      "paediatric + anaesthesia\n",
      "[14348]\n",
      "----------------------------------------------------\n",
      "sleep apea syndrome + chiari malformation\n",
      "[12307, 12307]\n",
      "----------------------------------------------------\n",
      "sleep apea syndrome + behaviour\n",
      "[12307, 12307]\n",
      "----------------------------------------------------\n",
      "heated humidification + humidity\n",
      "[]\n",
      "----------------------------------------------------\n",
      "car accident + excessive daytime sleepiness\n",
      "[]\n",
      "----------------------------------------------------\n",
      "management + excessive daytime sleepiness\n",
      "[8211, 8211]\n",
      "----------------------------------------------------\n",
      "outcome + surgery\n",
      "[10266, 10266, 10266, 10266]\n",
      "----------------------------------------------------\n",
      "outcome + meta analysis\n",
      "[10266, 10266, 10266, 10266]\n",
      "----------------------------------------------------\n",
      "sleep structure + schizophrenia\n",
      "[16413]\n",
      "----------------------------------------------------\n",
      "sleep structure + depression\n",
      "[16413]\n",
      "----------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for i,j in train_data[(train_data['label']==1)][0:20].iterrows():\n",
    "    print (node_label_find(key_list,j[0][0]),\"+\",node_label_find(key_list,j[0][1]))\n",
    "    t1=[]\n",
    "    t2=[]\n",
    "    for n1 in g_train[j[0][0]]:\n",
    "        t1.append(j[0][0])\n",
    "    for n2 in g_train[j[0][1]]:\n",
    "        t2.append(g_train[j[0][1]][n2]['title'])\n",
    "    print(t1)\n",
    "    print(\"----------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AtlasView({5741: {'art_id': {106857}, 'year': {2001}, 'title': {'Mortised genioplasty in the treatment of obstructive sleep apnea An historical perspective and modification of design'}, 'weight': 1}, 13102: {'art_id': {106857}, 'year': {2001}, 'title': {'Mortised genioplasty in the treatment of obstructive sleep apnea An historical perspective and modification of design'}, 'weight': 1}})"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g_train[10248]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sm_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-124-491fac12c5f9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msm_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'art_year'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'sm_df' is not defined"
     ]
    }
   ],
   "source": [
    "sm_df.groupby('art_year').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'pyemd' from 'C:\\\\Users\\\\faisal\\\\Anaconda3\\\\lib\\\\site-packages\\\\pyemd\\\\__init__.py'>"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "from pyemd import emd\n",
    "from gensim import corpora\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.matutils import softcossim\n",
    "import importlib\n",
    "importlib.reload(pyemd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors = KeyedVectors.load_word2vec_format('pubmed2018_w2v_200D/pubmed2018_w2v_200D.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_obama = 'hi world'.lower().split()\n",
    "sentence_president = 'hellow world'.lower().split()\n",
    "sentence_orange = 'congenital central hypoventilation syndrome'.lower().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [sentence_obama, sentence_president, sentence_orange]\n",
    "dictionary = corpora.Dictionary(documents)\n",
    "corpus = [dictionary.doc2bow(document) for document in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the sentences into bag-of-words vectors.\n",
    "sentence_obama = dictionary.doc2bow(sentence_obama)\n",
    "sentence_president = dictionary.doc2bow(sentence_president)\n",
    "sentence_orange = dictionary.doc2bow(sentence_orange)\n",
    "similarity_matrix = word_vectors.similarity_matrix(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "softcossim() missing 1 required positional argument: 'similarity_matrix'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-163-73ebc188ff8c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msimilarity\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoftcossim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence_obama\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msentence_president\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'similarity = %.4f'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0msimilarity\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: softcossim() missing 1 required positional argument: 'similarity_matrix'"
     ]
    }
   ],
   "source": [
    "similarity = softcossim(sentence_obama, sentence_president, similarity_matrix)\n",
    "print('similarity = %.4f' % similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Please install pyemd Python package to compute WMD.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-171-7e88f3f575b3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mword_vectors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwmdistance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'sleepiness'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'insomnia'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mwmdistance\u001b[1;34m(self, document1, document2)\u001b[0m\n\u001b[0;32m    578\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    579\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mPYEMD_EXT\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 580\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Please install pyemd Python package to compute WMD.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    581\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    582\u001b[0m         \u001b[1;31m# Remove out-of-vocabulary words.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: Please install pyemd Python package to compute WMD."
     ]
    }
   ],
   "source": [
    "word_vectors.wmdistance('sleepiness', 'insomnia')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7.0\n"
     ]
    }
   ],
   "source": [
    "from platform import python_version\n",
    "print(python_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7755101093935269"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors.n_similarity('sleepiness'.lower().split(), 'insomnia'.lower().split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "import temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import theano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-88d96843a926>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mabsolute_import\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mactivations\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mapplications\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\utils\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdata_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mio_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mconv_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m# Globally-importable utils.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\utils\\conv_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmoves\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbackend\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\backend\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[1;32melif\u001b[0m \u001b[0m_BACKEND\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'tensorflow'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m     \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Using TensorFlow backend.\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 89\u001b[1;33m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mtensorflow_backend\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     90\u001b[0m \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m     \u001b[1;31m# Try and load external backend.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mprint_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mops\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf_ops\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmoving_averages\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'keras' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-779d907b2a07>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mimportlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'keras' is not defined"
     ]
    }
   ],
   "source": [
    "importlib.reload(keras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-88d96843a926>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mabsolute_import\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mactivations\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mapplications\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\utils\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdata_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mio_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mconv_utils\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m# Globally-importable utils.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\utils\\conv_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmoves\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbackend\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\backend\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[1;32melif\u001b[0m \u001b[0m_BACKEND\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'tensorflow'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m     \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Using TensorFlow backend.\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 89\u001b[1;33m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mtensorflow_backend\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     90\u001b[0m \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m     \u001b[1;31m# Try and load external backend.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mprint_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mops\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf_ops\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmoving_averages\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Get some time series data\n",
    "df = pd.read_csv(\"https://raw.githubusercontent.com/plotly/datasets/master/timeseries.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "      <th>D</th>\n",
       "      <th>E</th>\n",
       "      <th>F</th>\n",
       "      <th>G</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2008-03-18</td>\n",
       "      <td>24.68</td>\n",
       "      <td>164.93</td>\n",
       "      <td>114.73</td>\n",
       "      <td>26.27</td>\n",
       "      <td>19.21</td>\n",
       "      <td>28.87</td>\n",
       "      <td>63.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2008-03-19</td>\n",
       "      <td>24.18</td>\n",
       "      <td>164.89</td>\n",
       "      <td>114.75</td>\n",
       "      <td>26.22</td>\n",
       "      <td>19.07</td>\n",
       "      <td>27.76</td>\n",
       "      <td>59.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2008-03-20</td>\n",
       "      <td>23.99</td>\n",
       "      <td>164.63</td>\n",
       "      <td>115.04</td>\n",
       "      <td>25.78</td>\n",
       "      <td>19.01</td>\n",
       "      <td>27.04</td>\n",
       "      <td>59.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2008-03-25</td>\n",
       "      <td>24.14</td>\n",
       "      <td>163.92</td>\n",
       "      <td>114.85</td>\n",
       "      <td>27.41</td>\n",
       "      <td>19.61</td>\n",
       "      <td>27.84</td>\n",
       "      <td>59.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2008-03-26</td>\n",
       "      <td>24.44</td>\n",
       "      <td>163.45</td>\n",
       "      <td>114.84</td>\n",
       "      <td>26.86</td>\n",
       "      <td>19.53</td>\n",
       "      <td>28.02</td>\n",
       "      <td>60.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2008-03-27</td>\n",
       "      <td>24.38</td>\n",
       "      <td>163.46</td>\n",
       "      <td>115.40</td>\n",
       "      <td>27.09</td>\n",
       "      <td>19.72</td>\n",
       "      <td>28.25</td>\n",
       "      <td>59.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2008-03-28</td>\n",
       "      <td>24.32</td>\n",
       "      <td>163.22</td>\n",
       "      <td>115.56</td>\n",
       "      <td>27.13</td>\n",
       "      <td>19.63</td>\n",
       "      <td>28.24</td>\n",
       "      <td>58.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2008-03-31</td>\n",
       "      <td>24.19</td>\n",
       "      <td>164.02</td>\n",
       "      <td>115.54</td>\n",
       "      <td>26.74</td>\n",
       "      <td>19.55</td>\n",
       "      <td>28.43</td>\n",
       "      <td>59.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2008-04-01</td>\n",
       "      <td>23.81</td>\n",
       "      <td>163.59</td>\n",
       "      <td>115.72</td>\n",
       "      <td>27.82</td>\n",
       "      <td>20.21</td>\n",
       "      <td>29.17</td>\n",
       "      <td>56.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2008-04-02</td>\n",
       "      <td>24.03</td>\n",
       "      <td>163.32</td>\n",
       "      <td>115.11</td>\n",
       "      <td>28.22</td>\n",
       "      <td>20.42</td>\n",
       "      <td>29.38</td>\n",
       "      <td>56.64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2008-04-03</td>\n",
       "      <td>24.34</td>\n",
       "      <td>163.34</td>\n",
       "      <td>115.17</td>\n",
       "      <td>28.14</td>\n",
       "      <td>20.36</td>\n",
       "      <td>29.51</td>\n",
       "      <td>57.49</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Date      A       B       C      D      E      F      G\n",
       "0   2008-03-18  24.68  164.93  114.73  26.27  19.21  28.87  63.44\n",
       "1   2008-03-19  24.18  164.89  114.75  26.22  19.07  27.76  59.98\n",
       "2   2008-03-20  23.99  164.63  115.04  25.78  19.01  27.04  59.61\n",
       "3   2008-03-25  24.14  163.92  114.85  27.41  19.61  27.84  59.41\n",
       "4   2008-03-26  24.44  163.45  114.84  26.86  19.53  28.02  60.09\n",
       "5   2008-03-27  24.38  163.46  115.40  27.09  19.72  28.25  59.62\n",
       "6   2008-03-28  24.32  163.22  115.56  27.13  19.63  28.24  58.65\n",
       "7   2008-03-31  24.19  164.02  115.54  26.74  19.55  28.43  59.20\n",
       "8   2008-04-01  23.81  163.59  115.72  27.82  20.21  29.17  56.18\n",
       "9   2008-04-02  24.03  163.32  115.11  28.22  20.42  29.38  56.64\n",
       "10  2008-04-03  24.34  163.34  115.17  28.14  20.36  29.51  57.49"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_cols = [\"A\",\"B\"] \n",
    "df['single_input_vector'] = df[['B','C']].apply(tuple, axis=1).apply(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['single_input_vector'] = df.single_input_vector.apply(lambda x: [list(x)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cumulative_input_vectors'] = df.single_input_vector.cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_cols=['B','C']\n",
    "df['output_vector'] = df[output_cols].apply(tuple, axis=1).apply(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "max_sequence_length = df.cumulative_input_vectors.apply(len).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_sequences = pad_sequences(df.cumulative_input_vectors.tolist(), max_sequence_length).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['padded_input_vectors'] = pd.Series(padded_sequences).apply(np.asarray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_init = np.asarray(df.padded_input_vectors)\n",
    "X_train = np.hstack(X_train_init).reshape(len(df),max_sequence_length,len(input_cols))\n",
    "y_train = np.hstack(np.asarray(df.output_vector)).reshape(len(df),len(output_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_length = X_train.shape[1]\n",
    "input_dim = X_train.shape[2]\n",
    "output_dim = len(y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model, Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "model = Sequential()\n",
    "model.add(LSTM(4, input_shape=(input_length,input_dim)))\n",
    "# The max output value is > 1 so relu is used as final activation.\n",
    "model.add(Dense(output_dim, activation='relu'))\n",
    "\n",
    "model.compile(loss='mean_squared_error',\n",
    "              optimizer='sgd',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2 samples, validate on 2 samples\n",
      "Epoch 1/170\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.5000 - acc: 0.5000 - val_loss: 0.5000 - val_acc: 0.5000\n",
      "Epoch 2/170\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.5000 - acc: 0.5000 - val_loss: 0.5000 - val_acc: 0.5000\n",
      "Epoch 3/170\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.5000 - acc: 0.5000 - val_loss: 0.5000 - val_acc: 0.5000\n",
      "Epoch 4/170\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.5000 - acc: 0.5000 - val_loss: 0.5000 - val_acc: 0.5000\n",
      "Epoch 5/170\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.5000 - acc: 0.5000 - val_loss: 0.5000 - val_acc: 0.5000\n",
      "Epoch 6/170\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.5000 - acc: 0.5000 - val_loss: 0.5000 - val_acc: 0.5000\n",
      "Epoch 7/170\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.5000 - acc: 0.5000 - val_loss: 0.5000 - val_acc: 0.5000\n",
      "Epoch 8/170\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.5000 - acc: 0.5000 - val_loss: 0.5000 - val_acc: 0.5000\n",
      "Epoch 9/170\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.5000 - acc: 0.5000 - val_loss: 0.5000 - val_acc: 0.5000\n",
      "Epoch 10/170\n",
      "2/2 [==============================] - 0s 997us/step - loss: 0.5000 - acc: 0.5000 - val_loss: 0.5000 - val_acc: 0.5000\n",
      "Epoch 11/170\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.5000 - acc: 0.5000 - val_loss: 0.5000 - val_acc: 0.5000\n",
      "Epoch 12/170\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.5000 - acc: 0.5000 - val_loss: 0.5000 - val_acc: 0.5000\n",
      "Epoch 13/170\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.5000 - acc: 0.5000 - val_loss: 0.5000 - val_acc: 0.5000\n",
      "Epoch 14/170\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.5000 - acc: 0.5000 - val_loss: 0.5000 - val_acc: 0.5000\n",
      "Epoch 15/170\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.5000 - acc: 0.5000 - val_loss: 0.5000 - val_acc: 0.5000\n",
      "Epoch 16/170\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.5000 - acc: 0.5000 - val_loss: 0.5000 - val_acc: 0.5000\n",
      "Epoch 17/170\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.5000 - acc: 0.5000 - val_loss: 0.5000 - val_acc: 0.5000\n",
      "Epoch 18/170\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.5000 - acc: 0.5000 - val_loss: 0.5000 - val_acc: 0.5000\n",
      "Epoch 19/170\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.5000 - acc: 0.5000 - val_loss: 0.5000 - val_acc: 0.5000\n",
      "Epoch 20/170\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.5000 - acc: 0.5000 - val_loss: 0.5000 - val_acc: 0.5000\n",
      "Epoch 21/170\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.5000 - acc: 0.5000 - val_loss: 0.5000 - val_acc: 0.5000\n",
      "Epoch 22/170\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.5000 - acc: 0.5000 - val_loss: 0.5000 - val_acc: 0.5000\n",
      "Epoch 23/170\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.5000 - acc: 0.5000 - val_loss: 0.5000 - val_acc: 0.5000\n",
      "Epoch 24/170\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.5000 - acc: 0.5000 - val_loss: 0.5000 - val_acc: 0.5000\n",
      "Epoch 25/170\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.5000 - acc: 0.5000 - val_loss: 0.5000 - val_acc: 0.5000\n",
      "Epoch 26/170\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.5000 - acc: 0.5000 - val_loss: 0.5000 - val_acc: 0.5000\n",
      "Epoch 27/170\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.5000 - acc: 0.5000 - val_loss: 0.5000 - val_acc: 0.5000\n",
      "Epoch 28/170\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.5000 - acc: 0.5000 - val_loss: 0.5000 - val_acc: 0.5000\n",
      "Epoch 29/170\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.5000 - acc: 0.5000 - val_loss: 0.5000 - val_acc: 0.5000\n",
      "Epoch 30/170\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.5000 - acc: 0.5000 - val_loss: 0.5000 - val_acc: 0.5000\n",
      "Epoch 31/170\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.5000 - acc: 0.5000 - val_loss: 0.5000 - val_acc: 0.5000\n",
      "Epoch 32/170\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.5000 - acc: 0.5000 - val_loss: 0.5000 - val_acc: 0.5000\n",
      "Epoch 33/170\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.5000 - acc: 0.5000 - val_loss: 0.5000 - val_acc: 0.5000\n",
      "Epoch 34/170\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.5000 - acc: 0.5000 - val_loss: 0.5000 - val_acc: 0.5000\n",
      "Epoch 35/170\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.5000 - acc: 0.5000 - val_loss: 0.5000 - val_acc: 0.5000\n",
      "Epoch 36/170\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.5000 - acc: 0.5000 - val_loss: 0.5000 - val_acc: 0.5000\n",
      "Epoch 37/170\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.5000 - acc: 0.5000 - val_loss: 0.5000 - val_acc: 0.5000\n",
      "Epoch 38/170\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.5000 - acc: 0.5000 - val_loss: 0.5000 - val_acc: 0.5000\n",
      "Epoch 39/170\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.5000 - acc: 0.5000 - val_loss: 0.5000 - val_acc: 0.5000\n",
      "Epoch 40/170\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.5000 - acc: 0.5000 - val_loss: 0.5000 - val_acc: 0.5000\n",
      "Epoch 41/170\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.5000 - acc: 0.5000 - val_loss: 0.5000 - val_acc: 0.5000\n",
      "Epoch 42/170\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.5000 - acc: 0.5000 - val_loss: 0.5000 - val_acc: 0.5000\n",
      "Epoch 43/170\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.5000 - acc: 0.5000 - val_loss: 0.5000 - val_acc: 0.5000\n",
      "Epoch 44/170\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.5000 - acc: 0.5000 - val_loss: 0.5000 - val_acc: 0.5000\n",
      "Epoch 45/170\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.5000 - acc: 0.5000 - val_loss: 0.5000 - val_acc: 0.5000\n",
      "Epoch 46/170\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.5000 - acc: 0.5000 - val_loss: 0.5000 - val_acc: 0.5000\n",
      "Epoch 47/170\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.5000 - acc: 0.5000 - val_loss: 0.5000 - val_acc: 0.5000\n",
      "Epoch 48/170\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.5000 - acc: 0.5000 - val_loss: 0.5000 - val_acc: 0.5000\n",
      "Epoch 49/170\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.5000 - acc: 0.5000 - val_loss: 0.5000 - val_acc: 0.5000\n",
      "Epoch 50/170\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.5000 - acc: 0.5000 - val_loss: 0.5000 - val_acc: 0.5000\n",
      "Epoch 51/170\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.5000 - acc: 0.5000 - val_loss: 0.5000 - val_acc: 0.5000\n",
      "Epoch 52/170\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.5000 - acc: 0.5000 - val_loss: 0.5000 - val_acc: 0.5000\n",
      "Epoch 53/170\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.5000 - acc: 0.5000 - val_loss: 0.5000 - val_acc: 0.5000\n",
      "Epoch 54/170\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.5000 - acc: 0.5000 - val_loss: 0.5000 - val_acc: 0.5000\n",
      "Epoch 55/170\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.5000 - acc: 0.5000 - val_loss: 0.5000 - val_acc: 0.5000\n",
      "Epoch 56/170\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.5000 - acc: 0.5000 - val_loss: 0.5000 - val_acc: 0.5000\n",
      "Epoch 57/170\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.5000 - acc: 0.5000 - val_loss: 0.5000 - val_acc: 0.5000\n",
      "Epoch 58/170\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.5000 - acc: 0.5000 - val_loss: 0.5000 - val_acc: 0.5000\n",
      "Epoch 59/170\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.5000 - acc: 0.5000 - val_loss: 0.5000 - val_acc: 0.5000\n",
      "Epoch 60/170\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.5000 - acc: 0.5000 - val_loss: 0.5000 - val_acc: 0.5000\n",
      "Epoch 61/170\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.5000 - acc: 0.5000 - val_loss: 0.5000 - val_acc: 0.5000\n",
      "Epoch 62/170\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.5000 - acc: 0.5000 - val_loss: 0.5000 - val_acc: 0.5000\n",
      "Epoch 63/170\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.5000 - acc: 0.5000 - val_loss: 0.5000 - val_acc: 0.5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64/170\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.5000 - acc: 0.5000 - val_loss: 0.5000 - val_acc: 0.5000\n",
      "Epoch 65/170\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.5000 - acc: 0.5000 - val_loss: 0.5000 - val_acc: 0.5000\n",
      "Epoch 66/170\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.5000 - acc: 0.5000 - val_loss: 0.5000 - val_acc: 0.5000\n",
      "Epoch 67/170\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.5000 - acc: 0.5000 - val_loss: 0.5000 - val_acc: 0.5000\n",
      "Epoch 68/170\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.5000 - acc: 0.5000 - val_loss: 0.5000 - val_acc: 0.5000\n",
      "Epoch 69/170\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.5000 - acc: 0.5000 - val_loss: 0.5000 - val_acc: 0.5000\n",
      "Epoch 70/170\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.5000 - acc: 0.5000 - val_loss: 0.5000 - val_acc: 0.5000\n",
      "Epoch 71/170\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.5000 - acc: 0.5000 - val_loss: 0.5000 - val_acc: 0.5000\n",
      "Epoch 72/170\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.5000 - acc: 0.5000 - val_loss: 0.5000 - val_acc: 0.5000\n",
      "Epoch 73/170\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.5000 - acc: 0.5000 - val_loss: 0.5000 - val_acc: 0.5000\n",
      "Epoch 74/170\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.5000 - acc: 0.5000 - val_loss: 0.5000 - val_acc: 0.5000\n",
      "Epoch 75/170\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.5000 - acc: 0.5000 - val_loss: 0.5000 - val_acc: 0.5000\n",
      "Epoch 76/170\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.5000 - acc: 0.5000 - val_loss: 0.5000 - val_acc: 0.5000\n",
      "Epoch 77/170\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.5000 - acc: 0.5000 - val_loss: 0.5000 - val_acc: 0.5000\n",
      "Epoch 78/170\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.5000 - acc: 0.5000 - val_loss: 0.5000 - val_acc: 0.5000\n",
      "Epoch 79/170\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.5000 - acc: 0.5000 - val_loss: 0.5000 - val_acc: 0.5000\n",
      "Epoch 80/170\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.5000 - acc: 0.5000 - val_loss: 0.5000 - val_acc: 0.5000\n",
      "Epoch 81/170\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.5000 - acc: 0.5000 - val_loss: 0.5000 - val_acc: 0.5000\n",
      "Epoch 82/170\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.5000 - acc: 0.5000 - val_loss: 0.5000 - val_acc: 0.5000\n",
      "Epoch 83/170\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.5000 - acc: 0.5000 - val_loss: 0.5000 - val_acc: 0.5000\n",
      "Epoch 84/170\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.5000 - acc: 0.5000 - val_loss: 0.5000 - val_acc: 0.5000\n",
      "Epoch 85/170\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.5000 - acc: 0.5000 - val_loss: 0.5000 - val_acc: 0.5000\n",
      "Epoch 86/170\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.5000 - acc: 0.5000 - val_loss: 0.5000 - val_acc: 0.5000\n",
      "Epoch 87/170\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.5000 - acc: 0.5000 - val_loss: 0.5000 - val_acc: 0.5000\n",
      "Epoch 88/170\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.5000 - acc: 0.5000 - val_loss: 0.5000 - val_acc: 0.5000\n",
      "Epoch 89/170\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.5000 - acc: 0.5000 - val_loss: 0.5000 - val_acc: 0.5000\n",
      "Epoch 90/170\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.5000 - acc: 0.5000 - val_loss: 0.5000 - val_acc: 0.5000\n",
      "Epoch 91/170\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.5000 - acc: 0.5000 - val_loss: 0.5000 - val_acc: 0.5000\n",
      "Epoch 92/170\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.5000 - acc: 0.5000 - val_loss: 0.5000 - val_acc: 0.5000\n",
      "Epoch 93/170\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.5000 - acc: 0.5000 - val_loss: 0.5000 - val_acc: 0.5000\n",
      "Epoch 94/170\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.5000 - acc: 0.5000 - val_loss: 0.5000 - val_acc: 0.5000\n",
      "Epoch 95/170\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.5000 - acc: 0.5000 - val_loss: 0.5000 - val_acc: 0.5000\n",
      "Epoch 96/170\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.5000 - acc: 0.5000 - val_loss: 0.5000 - val_acc: 0.5000\n",
      "Epoch 97/170\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.5000 - acc: 0.5000 - val_loss: 0.5000 - val_acc: 0.5000\n",
      "Epoch 98/170\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.5000 - acc: 0.5000 - val_loss: 0.5000 - val_acc: 0.5000\n",
      "Epoch 99/170\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.5000 - acc: 0.5000 - val_loss: 0.5000 - val_acc: 0.5000\n",
      "Epoch 100/170\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.5000 - acc: 0.5000 - val_loss: 0.5000 - val_acc: 0.5000\n",
      "Epoch 101/170\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.5000 - acc: 0.5000 - val_loss: 0.5000 - val_acc: 0.5000\n",
      "Epoch 102/170\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.5000 - acc: 0.5000 - val_loss: 0.5000 - val_acc: 0.5000\n",
      "Epoch 103/170\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.5000 - acc: 0.5000 - val_loss: 0.5000 - val_acc: 0.5000\n",
      "Epoch 104/170\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.5000 - acc: 0.5000 - val_loss: 0.5000 - val_acc: 0.5000\n",
      "Epoch 105/170\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.5000 - acc: 0.5000 - val_loss: 0.5000 - val_acc: 0.5000\n",
      "Epoch 106/170\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.5000 - acc: 0.5000 - val_loss: 0.5000 - val_acc: 0.5000\n",
      "Epoch 107/170\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.5000 - acc: 0.5000 - val_loss: 0.5000 - val_acc: 0.5000\n",
      "Epoch 108/170\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.5000 - acc: 0.5000 - val_loss: 0.5000 - val_acc: 0.5000\n",
      "Epoch 109/170\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.5000 - acc: 0.5000 - val_loss: 0.5000 - val_acc: 0.5000\n",
      "Epoch 110/170\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.5000 - acc: 0.5000 - val_loss: 0.5000 - val_acc: 0.5000\n",
      "Epoch 111/170\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.5000 - acc: 0.5000 - val_loss: 0.5000 - val_acc: 0.5000\n",
      "Epoch 112/170\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.5000 - acc: 0.5000 - val_loss: 0.5000 - val_acc: 0.5000\n",
      "Epoch 113/170\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.5000 - acc: 0.5000 - val_loss: 0.5000 - val_acc: 0.5000\n",
      "Epoch 114/170\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.5000 - acc: 0.5000 - val_loss: 0.5000 - val_acc: 0.5000\n",
      "Epoch 115/170\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.5000 - acc: 0.5000 - val_loss: 0.5000 - val_acc: 0.5000\n",
      "Epoch 116/170\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.5000 - acc: 0.5000 - val_loss: 0.5000 - val_acc: 0.5000\n",
      "Epoch 117/170\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.5000 - acc: 0.5000 - val_loss: 0.5000 - val_acc: 0.5000\n",
      "Epoch 118/170\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.5000 - acc: 0.5000 - val_loss: 0.5000 - val_acc: 0.5000\n",
      "Epoch 119/170\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.5000 - acc: 0.5000 - val_loss: 0.5000 - val_acc: 0.5000\n",
      "Epoch 120/170\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.5000 - acc: 0.5000 - val_loss: 0.5000 - val_acc: 0.5000\n",
      "Epoch 121/170\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.5000 - acc: 0.5000 - val_loss: 0.5000 - val_acc: 0.5000\n",
      "Epoch 122/170\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.5000 - acc: 0.5000 - val_loss: 0.5000 - val_acc: 0.5000\n",
      "Epoch 123/170\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.5000 - acc: 0.5000 - val_loss: 0.5000 - val_acc: 0.5000\n",
      "Epoch 124/170\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.5000 - acc: 0.5000 - val_loss: 0.5000 - val_acc: 0.5000\n",
      "Epoch 125/170\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.5000 - acc: 0.5000 - val_loss: 0.5000 - val_acc: 0.5000\n",
      "Epoch 126/170\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.5000 - acc: 0.5000 - val_loss: 0.5000 - val_acc: 0.5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 127/170\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.5000 - acc: 0.5000 - val_loss: 0.5000 - val_acc: 0.5000\n",
      "Epoch 128/170\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.5000 - acc: 0.5000 - val_loss: 0.5000 - val_acc: 0.5000\n",
      "Epoch 129/170\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.5000 - acc: 0.5000 - val_loss: 0.5000 - val_acc: 0.5000\n",
      "Epoch 130/170\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.5000 - acc: 0.5000 - val_loss: 0.5000 - val_acc: 0.5000\n",
      "Epoch 131/170\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.5000 - acc: 0.5000 - val_loss: 0.5000 - val_acc: 0.5000\n",
      "Epoch 132/170\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.5000 - acc: 0.5000 - val_loss: 0.5000 - val_acc: 0.5000\n",
      "Epoch 133/170\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.5000 - acc: 0.5000 - val_loss: 0.5000 - val_acc: 0.5000\n",
      "Epoch 134/170\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.5000 - acc: 0.5000 - val_loss: 0.5000 - val_acc: 0.5000\n",
      "Epoch 135/170\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.5000 - acc: 0.5000 - val_loss: 0.5000 - val_acc: 0.5000\n",
      "Epoch 136/170\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.5000 - acc: 0.5000 - val_loss: 0.5000 - val_acc: 0.5000\n",
      "Epoch 137/170\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.5000 - acc: 0.5000 - val_loss: 0.5000 - val_acc: 0.5000\n",
      "Epoch 138/170\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.5000 - acc: 0.5000 - val_loss: 0.5000 - val_acc: 0.5000\n",
      "Epoch 139/170\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.5000 - acc: 0.5000 - val_loss: 0.5000 - val_acc: 0.5000\n",
      "Epoch 140/170\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.5000 - acc: 0.5000 - val_loss: 0.5000 - val_acc: 0.5000\n",
      "Epoch 141/170\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.5000 - acc: 0.5000 - val_loss: 0.5000 - val_acc: 0.5000\n",
      "Epoch 142/170\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.5000 - acc: 0.5000 - val_loss: 0.5000 - val_acc: 0.5000\n",
      "Epoch 143/170\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.5000 - acc: 0.5000 - val_loss: 0.5000 - val_acc: 0.5000\n",
      "Epoch 144/170\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.5000 - acc: 0.5000 - val_loss: 0.5000 - val_acc: 0.5000\n",
      "Epoch 145/170\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.5000 - acc: 0.5000 - val_loss: 0.5000 - val_acc: 0.5000\n",
      "Epoch 146/170\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.5000 - acc: 0.5000 - val_loss: 0.5000 - val_acc: 0.5000\n",
      "Epoch 147/170\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.5000 - acc: 0.5000 - val_loss: 0.5000 - val_acc: 0.5000\n",
      "Epoch 148/170\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.5000 - acc: 0.5000 - val_loss: 0.5000 - val_acc: 0.5000\n",
      "Epoch 149/170\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.5000 - acc: 0.5000 - val_loss: 0.5000 - val_acc: 0.5000\n",
      "Epoch 150/170\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.5000 - acc: 0.5000 - val_loss: 0.5000 - val_acc: 0.5000\n",
      "Epoch 151/170\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.5000 - acc: 0.5000 - val_loss: 0.5000 - val_acc: 0.5000\n",
      "Epoch 152/170\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.5000 - acc: 0.5000 - val_loss: 0.5000 - val_acc: 0.5000\n",
      "Epoch 153/170\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.5000 - acc: 0.5000 - val_loss: 0.5000 - val_acc: 0.5000\n",
      "Epoch 154/170\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.5000 - acc: 0.5000 - val_loss: 0.5000 - val_acc: 0.5000\n",
      "Epoch 155/170\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.5000 - acc: 0.5000 - val_loss: 0.5000 - val_acc: 0.5000\n",
      "Epoch 156/170\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.5000 - acc: 0.5000 - val_loss: 0.5000 - val_acc: 0.5000\n",
      "Epoch 157/170\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.5000 - acc: 0.5000 - val_loss: 0.5000 - val_acc: 0.5000\n",
      "Epoch 158/170\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.5000 - acc: 0.5000 - val_loss: 0.5000 - val_acc: 0.5000\n",
      "Epoch 159/170\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.5000 - acc: 0.5000 - val_loss: 0.5000 - val_acc: 0.5000\n",
      "Epoch 160/170\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.5000 - acc: 0.5000 - val_loss: 0.5000 - val_acc: 0.5000\n",
      "Epoch 161/170\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.5000 - acc: 0.5000 - val_loss: 0.5000 - val_acc: 0.5000\n",
      "Epoch 162/170\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.5000 - acc: 0.5000 - val_loss: 0.5000 - val_acc: 0.5000\n",
      "Epoch 163/170\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.5000 - acc: 0.5000 - val_loss: 0.5000 - val_acc: 0.5000\n",
      "Epoch 164/170\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.5000 - acc: 0.5000 - val_loss: 0.5000 - val_acc: 0.5000\n",
      "Epoch 165/170\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.5000 - acc: 0.5000 - val_loss: 0.5000 - val_acc: 0.5000\n",
      "Epoch 166/170\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.5000 - acc: 0.5000 - val_loss: 0.5000 - val_acc: 0.5000\n",
      "Epoch 167/170\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.5000 - acc: 0.5000 - val_loss: 0.5000 - val_acc: 0.5000\n",
      "Epoch 168/170\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.5000 - acc: 0.5000 - val_loss: 0.5000 - val_acc: 0.5000\n",
      "Epoch 169/170\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.5000 - acc: 0.5000 - val_loss: 0.5000 - val_acc: 0.5000\n",
      "Epoch 170/170\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.5000 - acc: 0.5000 - val_loss: 0.5000 - val_acc: 0.5000\n"
     ]
    }
   ],
   "source": [
    "#Set batch_size to 7 to show that it doesn't have to be a factor or multiple of your sample size\n",
    "history = model.fit(X_train, y_train,validation_split=0.3,\n",
    "              batch_size=7, epochs=170,\n",
    "              verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[163.71799 , 115.03436 ],\n",
       "       [163.71896 , 115.03505 ],\n",
       "       [163.71928 , 115.03526 ],\n",
       "       [163.71939 , 115.03534 ],\n",
       "       [163.71942 , 115.03537 ],\n",
       "       [163.71944 , 115.03538 ],\n",
       "       [163.71944 , 115.03538 ],\n",
       "       [163.71945 , 115.035385],\n",
       "       [163.71945 , 115.035385],\n",
       "       [163.71945 , 115.035385],\n",
       "       [163.71945 , 115.035385]], dtype=float32)"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[164.93, 114.73],\n",
       "       [164.89, 114.75],\n",
       "       [164.63, 115.04],\n",
       "       [163.92, 114.85],\n",
       "       [163.45, 114.84],\n",
       "       [163.46, 115.4 ],\n",
       "       [163.22, 115.56],\n",
       "       [164.02, 115.54],\n",
       "       [163.59, 115.72],\n",
       "       [163.32, 115.11],\n",
       "       [163.34, 115.17]])"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=np.array([\n",
    "\t[\n",
    "\t\t[\n",
    "\t\t\t1,3,4,1\n",
    "\t\t],\n",
    "\t\t[\n",
    "\t\t\t3,2,1,2\n",
    "\t\t]\n",
    "\t],\n",
    "\t[\n",
    "\t\t[\n",
    "\t\t\t4,1,5,4\n",
    "\t\t],\n",
    "\t\t[\n",
    "\t\t\t6,7,2,1\n",
    "\t\t]\n",
    "\t],\n",
    "\t[\n",
    "\t\t[\n",
    "\t\t\t5,8,1,7\n",
    "\t\t],\n",
    "\t\t[\n",
    "\t\t\t9,1,8,4\n",
    "\t\t]\n",
    "\t],\n",
    "    [\n",
    "\t\t[\n",
    "\t\t\t2,3,4,1\n",
    "\t\t],\n",
    "\t\t[\n",
    "\t\t\t5,2,1,2\n",
    "\t\t]\n",
    "\t],\n",
    "\t[\n",
    "\t\t[\n",
    "\t\t\t4,1,5,4\n",
    "\t\t],\n",
    "\t\t[\n",
    "\t\t\t6,7,2,1\n",
    "\t\t]\n",
    "\t],\n",
    "\t[\n",
    "\t\t[\n",
    "\t\t\t9,8,1,7\n",
    "\t\t],\n",
    "\t\t[\n",
    "\t\t\t1,1,2,4\n",
    "\t\t]\n",
    "\t]\n",
    "])\n",
    "\n",
    "y=np.array([0,0,0,1,0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size = 0.3, random_state = 0)\n",
    "input_length = X_train.shape[1]\n",
    "input_dim = X_train.shape[2]\n",
    "output_dim = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(4, input_shape=(input_length,input_dim)))\n",
    "# The max output value is > 1 so relu is used as final activation.\n",
    "model.add(Dense(output_dim, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='Adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2 samples, validate on 2 samples\n",
      "Epoch 1/70\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.5555 - acc: 1.0000 - val_loss: 0.7462 - val_acc: 0.5000\n",
      "Epoch 2/70\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.5539 - acc: 1.0000 - val_loss: 0.7452 - val_acc: 0.5000\n",
      "Epoch 3/70\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.5523 - acc: 1.0000 - val_loss: 0.7442 - val_acc: 0.5000\n",
      "Epoch 4/70\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.5507 - acc: 1.0000 - val_loss: 0.7431 - val_acc: 0.5000\n",
      "Epoch 5/70\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.5491 - acc: 1.0000 - val_loss: 0.7420 - val_acc: 0.5000\n",
      "Epoch 6/70\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.5476 - acc: 1.0000 - val_loss: 0.7410 - val_acc: 0.5000\n",
      "Epoch 7/70\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.5461 - acc: 1.0000 - val_loss: 0.7399 - val_acc: 0.5000\n",
      "Epoch 8/70\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.5446 - acc: 1.0000 - val_loss: 0.7388 - val_acc: 0.5000\n",
      "Epoch 9/70\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.5432 - acc: 1.0000 - val_loss: 0.7377 - val_acc: 0.5000\n",
      "Epoch 10/70\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.5417 - acc: 1.0000 - val_loss: 0.7366 - val_acc: 0.5000\n",
      "Epoch 11/70\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.5402 - acc: 1.0000 - val_loss: 0.7355 - val_acc: 0.5000\n",
      "Epoch 12/70\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.5388 - acc: 1.0000 - val_loss: 0.7344 - val_acc: 0.5000\n",
      "Epoch 13/70\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.5373 - acc: 1.0000 - val_loss: 0.7333 - val_acc: 0.5000\n",
      "Epoch 14/70\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.5359 - acc: 1.0000 - val_loss: 0.7322 - val_acc: 0.5000\n",
      "Epoch 15/70\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.5345 - acc: 1.0000 - val_loss: 0.7310 - val_acc: 0.5000\n",
      "Epoch 16/70\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.5330 - acc: 1.0000 - val_loss: 0.7299 - val_acc: 0.5000\n",
      "Epoch 17/70\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.5316 - acc: 1.0000 - val_loss: 0.7287 - val_acc: 0.5000\n",
      "Epoch 18/70\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.5302 - acc: 1.0000 - val_loss: 0.7275 - val_acc: 0.5000\n",
      "Epoch 19/70\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.5288 - acc: 1.0000 - val_loss: 0.7263 - val_acc: 0.5000\n",
      "Epoch 20/70\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.5274 - acc: 1.0000 - val_loss: 0.7250 - val_acc: 0.5000\n",
      "Epoch 21/70\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.5260 - acc: 1.0000 - val_loss: 0.7238 - val_acc: 0.5000\n",
      "Epoch 22/70\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.5246 - acc: 1.0000 - val_loss: 0.7225 - val_acc: 0.5000\n",
      "Epoch 23/70\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.5232 - acc: 1.0000 - val_loss: 0.7212 - val_acc: 0.5000\n",
      "Epoch 24/70\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.5218 - acc: 1.0000 - val_loss: 0.7199 - val_acc: 0.5000\n",
      "Epoch 25/70\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.5204 - acc: 1.0000 - val_loss: 0.7186 - val_acc: 0.5000\n",
      "Epoch 26/70\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.5190 - acc: 1.0000 - val_loss: 0.7172 - val_acc: 0.5000\n",
      "Epoch 27/70\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.5176 - acc: 1.0000 - val_loss: 0.7159 - val_acc: 0.5000\n",
      "Epoch 28/70\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.5163 - acc: 1.0000 - val_loss: 0.7147 - val_acc: 0.5000\n",
      "Epoch 29/70\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.5149 - acc: 1.0000 - val_loss: 0.7133 - val_acc: 0.5000\n",
      "Epoch 30/70\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.5136 - acc: 1.0000 - val_loss: 0.7121 - val_acc: 0.5000\n",
      "Epoch 31/70\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.5124 - acc: 1.0000 - val_loss: 0.7109 - val_acc: 0.5000\n",
      "Epoch 32/70\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.5111 - acc: 1.0000 - val_loss: 0.7096 - val_acc: 0.5000\n",
      "Epoch 33/70\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.5098 - acc: 1.0000 - val_loss: 0.7084 - val_acc: 0.5000\n",
      "Epoch 34/70\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.5085 - acc: 1.0000 - val_loss: 0.7072 - val_acc: 0.5000\n",
      "Epoch 35/70\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.5072 - acc: 1.0000 - val_loss: 0.7061 - val_acc: 0.5000\n",
      "Epoch 36/70\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.5059 - acc: 1.0000 - val_loss: 0.7049 - val_acc: 0.5000\n",
      "Epoch 37/70\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.5046 - acc: 1.0000 - val_loss: 0.7037 - val_acc: 0.5000\n",
      "Epoch 38/70\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.5033 - acc: 1.0000 - val_loss: 0.7026 - val_acc: 0.5000\n",
      "Epoch 39/70\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.5019 - acc: 1.0000 - val_loss: 0.7014 - val_acc: 0.5000\n",
      "Epoch 40/70\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.5006 - acc: 1.0000 - val_loss: 0.7003 - val_acc: 0.5000\n",
      "Epoch 41/70\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.4993 - acc: 1.0000 - val_loss: 0.6993 - val_acc: 0.5000\n",
      "Epoch 42/70\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.4980 - acc: 1.0000 - val_loss: 0.6983 - val_acc: 0.5000\n",
      "Epoch 43/70\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.4968 - acc: 1.0000 - val_loss: 0.6973 - val_acc: 0.5000\n",
      "Epoch 44/70\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.4955 - acc: 1.0000 - val_loss: 0.6963 - val_acc: 0.5000\n",
      "Epoch 45/70\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 0.4942 - acc: 1.0000 - val_loss: 0.6953 - val_acc: 0.5000\n",
      "Epoch 46/70\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.4930 - acc: 1.0000 - val_loss: 0.6943 - val_acc: 0.5000\n",
      "Epoch 47/70\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.4917 - acc: 1.0000 - val_loss: 0.6933 - val_acc: 0.5000\n",
      "Epoch 48/70\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.4904 - acc: 1.0000 - val_loss: 0.6924 - val_acc: 0.5000\n",
      "Epoch 49/70\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.4892 - acc: 1.0000 - val_loss: 0.6914 - val_acc: 0.5000\n",
      "Epoch 50/70\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.4879 - acc: 1.0000 - val_loss: 0.6905 - val_acc: 0.5000\n",
      "Epoch 51/70\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.4866 - acc: 1.0000 - val_loss: 0.6895 - val_acc: 0.5000\n",
      "Epoch 52/70\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.4853 - acc: 1.0000 - val_loss: 0.6886 - val_acc: 0.5000\n",
      "Epoch 53/70\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.4841 - acc: 1.0000 - val_loss: 0.6877 - val_acc: 0.5000\n",
      "Epoch 54/70\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.4828 - acc: 1.0000 - val_loss: 0.6868 - val_acc: 0.5000\n",
      "Epoch 55/70\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.4815 - acc: 1.0000 - val_loss: 0.6860 - val_acc: 0.5000\n",
      "Epoch 56/70\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.4803 - acc: 1.0000 - val_loss: 0.6852 - val_acc: 0.5000\n",
      "Epoch 57/70\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.4791 - acc: 1.0000 - val_loss: 0.6844 - val_acc: 0.5000\n",
      "Epoch 58/70\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.4779 - acc: 1.0000 - val_loss: 0.6836 - val_acc: 0.5000\n",
      "Epoch 59/70\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.4766 - acc: 1.0000 - val_loss: 0.6829 - val_acc: 0.5000\n",
      "Epoch 60/70\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.4754 - acc: 1.0000 - val_loss: 0.6821 - val_acc: 0.5000\n",
      "Epoch 61/70\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.4741 - acc: 1.0000 - val_loss: 0.6815 - val_acc: 0.5000\n",
      "Epoch 62/70\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.4729 - acc: 1.0000 - val_loss: 0.6808 - val_acc: 0.5000\n",
      "Epoch 63/70\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.4717 - acc: 1.0000 - val_loss: 0.6801 - val_acc: 0.5000\n",
      "Epoch 64/70\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.4704 - acc: 1.0000 - val_loss: 0.6794 - val_acc: 0.5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65/70\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.4692 - acc: 1.0000 - val_loss: 0.6788 - val_acc: 0.5000\n",
      "Epoch 66/70\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.4680 - acc: 1.0000 - val_loss: 0.6781 - val_acc: 0.5000\n",
      "Epoch 67/70\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.4667 - acc: 1.0000 - val_loss: 0.6774 - val_acc: 0.5000\n",
      "Epoch 68/70\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 0.4655 - acc: 1.0000 - val_loss: 0.6768 - val_acc: 0.5000\n",
      "Epoch 69/70\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.4642 - acc: 1.0000 - val_loss: 0.6762 - val_acc: 0.5000\n",
      "Epoch 70/70\n",
      "2/2 [==============================] - 0s 1ms/step - loss: 0.4630 - acc: 1.0000 - val_loss: 0.6756 - val_acc: 0.5000\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train,validation_split=0.3,\n",
    "              batch_size=7, epochs=70,\n",
    "              verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5806007 ],\n",
       "       [0.42457026]], dtype=float32)"
      ]
     },
     "execution_count": 340,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
